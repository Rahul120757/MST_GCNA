{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2_dGMp5mzcjy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, GRU, Dropout, Layer\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import KNNImputer\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "data = pd.read_csv(\"Clearness_Index.csv\")"
      ],
      "metadata": {
        "id": "kBT1l8rfz8E-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert date column to datetime format\n",
        "data['DATE'] = pd.to_datetime(data['DATE'], format='%d-%m-%Y %H:%M')"
      ],
      "metadata": {
        "id": "HD5_KFnK0VFm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing values\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "data_imputed = pd.DataFrame(imputer.fit_transform(data.drop(columns=['DATE'])), columns=data.columns[1:])\n"
      ],
      "metadata": {
        "id": "mJ6i5wen0VAE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Attention Layer\n",
        "class AttentionLayer(Layer):\n",
        "    def __init__(self):\n",
        "        super(AttentionLayer, self).__init__()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W = self.add_weight(shape=(input_shape[-1], 1), initializer='random_normal', trainable=True)\n",
        "        self.b = self.add_weight(shape=(1,), initializer='zeros', trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        score = tf.matmul(inputs, self.W) + self.b\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        context_vector = attention_weights * inputs\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        return context_vector"
      ],
      "metadata": {
        "id": "EkZuWCWZ1EJt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GCN Layer\n",
        "class GCNLayer(Layer):\n",
        "    def __init__(self, out_features):\n",
        "        super(GCNLayer, self).__init__()\n",
        "        self.dense = Dense(out_features, activation='relu')\n",
        "\n",
        "    def call(self, inputs, adj_matrix):\n",
        "        x = self.dense(inputs)  # Apply linear transformation\n",
        "        x = tf.matmul(adj_matrix, x)  # Apply adjacency matrix\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "_tqKyVXQ1LhF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GRU Layer\n",
        "class GRULayer(Layer):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(GRULayer, self).__init__()\n",
        "        self.gru = GRU(hidden_size, return_sequences=False)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        output = self.gru(inputs)  # Apply GRU\n",
        "        return output"
      ],
      "metadata": {
        "id": "R8n7ElS61SmH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-Attribute Spatial Temporal Graph Convolutional Network\n",
        "class MSTGCN(Model):\n",
        "    def __init__(self, num_features, gcn_hidden_dim, gru_hidden_dim, output_dim, dropout_rate):\n",
        "        super(MSTGCN, self).__init__()\n",
        "        self.gcn_layer = GCNLayer(gcn_hidden_dim)\n",
        "        self.attention_layer = AttentionLayer()\n",
        "        self.gru_layer = GRULayer(gru_hidden_dim)\n",
        "        self.dropout = Dropout(dropout_rate)\n",
        "        self.fc = Dense(output_dim)\n",
        "\n",
        "    def call(self, inputs, adj_matrix):\n",
        "        spatial_features = self.gcn_layer(inputs, adj_matrix)\n",
        "        attention_features = self.attention_layer(spatial_features)\n",
        "        temporal_features = self.gru_layer(tf.expand_dims(attention_features, axis=1))\n",
        "        temporal_features = self.dropout(temporal_features)\n",
        "        output = self.fc(temporal_features)\n",
        "        return output"
      ],
      "metadata": {
        "id": "09jzF6iR1YDr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add lags\n",
        "window_size = 10\n",
        "lagged_data = []\n",
        "for i in range(len(data_imputed) - window_size):\n",
        "    lagged_data.append(data_imputed.iloc[i:i + window_size].values)\n",
        "lagged_data = np.array(lagged_data)"
      ],
      "metadata": {
        "id": "2xCNLkr51pPN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract features and target\n",
        "features = lagged_data[:, :-1, :]\n",
        "target = lagged_data[:, -1, -1].reshape(-1, 1)"
      ],
      "metadata": {
        "id": "VFKrsNBB1uGo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "features_scaled = scaler.fit_transform(features.reshape(-1, features.shape[-1])).reshape(features.shape)\n",
        "target_scaled = scaler.fit_transform(target)"
      ],
      "metadata": {
        "id": "70zQVyOb1yYX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into 80% training and 20% testing\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(features_scaled, target_scaled, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "2NC8Hux112Vb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model parameters\n",
        "input_dim = features.shape[2]\n",
        "gcn_hidden_dim = 288\n",
        "gru_hidden_dim = 288\n",
        "output_dim = 1\n",
        "dropout_rate = 0.5\n",
        "num_epochs = 200\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "0RMdKLW72AEo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning rate scheduler\n",
        "initial_learning_rate = 0.0005\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=initial_learning_rate,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.96,\n",
        "    staircase=True\n",
        ")"
      ],
      "metadata": {
        "id": "LPAqO5xI58Ye"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the KFold cross-validator\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n"
      ],
      "metadata": {
        "id": "bPVTEn5I5j8J"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare to collect the cross-validation results\n",
        "val_losses = []\n",
        "val_maes = []\n",
        "val_rmses = []"
      ],
      "metadata": {
        "id": "OljV6c9e5m_d"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-validation loop\n",
        "for train_index, val_index in kf.split(X_train_full):\n",
        "    X_train, X_val = X_train_full[train_index], X_train_full[val_index]\n",
        "    y_train, y_val = y_train_full[train_index], y_train_full[val_index]\n",
        "\n",
        "    # Convert to TensorFlow datasets\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size)\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(batch_size)\n",
        "\n",
        "    # Initialize MSTGCN model\n",
        "    model = MSTGCN(input_dim, gcn_hidden_dim, gru_hidden_dim, output_dim, dropout_rate)\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=optimizer, loss=loss_fn)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        for X_batch, y_batch in train_dataset:\n",
        "            adj_matrix_train = tf.eye(X_batch.shape[1])  # Update adjacency matrix shape to match sequence length\n",
        "            with tf.GradientTape() as tape:\n",
        "                outputs = model(X_batch, adj_matrix_train)\n",
        "                loss = loss_fn(y_batch, outputs)\n",
        "            gradients = tape.gradient(loss, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "        # Validation\n",
        "        val_loss = 0\n",
        "        val_predictions = []\n",
        "        val_targets = []\n",
        "        for X_batch, y_batch in val_dataset:\n",
        "            adj_matrix_val = tf.eye(X_batch.shape[1])  # Update adjacency matrix shape to match sequence length\n",
        "            val_outputs = model(X_batch, adj_matrix_val)\n",
        "            val_loss += loss_fn(y_batch, val_outputs).numpy()\n",
        "            val_predictions.append(val_outputs.numpy())\n",
        "            val_targets.append(y_batch.numpy())\n",
        "\n",
        "        val_loss /= len(val_dataset)\n",
        "        val_predictions = np.concatenate(val_predictions, axis=0)\n",
        "        val_targets = np.concatenate(val_targets, axis=0)\n",
        "        val_mae = mean_absolute_error(val_targets, val_predictions)\n",
        "        val_rmse = np.sqrt(mean_squared_error(val_targets, val_predictions))\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.numpy()}, Val Loss: {val_loss}, Val MAE: {val_mae}, Val RMSE: {val_rmse}')\n",
        "\n",
        "    val_losses.append(val_loss)\n",
        "    val_maes.append(val_mae)\n",
        "    val_rmses.append(val_rmse)\n",
        "\n",
        "print(\"Cross-Validation Results\")\n",
        "print(\"Average Val Loss:\", np.mean(val_losses))\n",
        "print(\"Average Val MAE:\", np.mean(val_maes))\n",
        "print(\"Average Val RMSE:\", np.mean(val_rmses))"
      ],
      "metadata": {
        "id": "QYRROsjC5qeq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2c83287-68e0-4e0e-a25b-d662a758b824"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200, Loss: 0.4412233829498291, Val Loss: 0.41058951306205266, Val MAE: 0.5308501465553971, Val RMSE: 0.6408783397529625\n",
            "Epoch 11/200, Loss: 0.17459988594055176, Val Loss: 0.13932360441698505, Val MAE: 0.2871544803334572, Val RMSE: 0.373408131881467\n",
            "Epoch 21/200, Loss: 0.12765821814537048, Val Loss: 0.10034026065572149, Val MAE: 0.2377447592337452, Val RMSE: 0.31701453492899156\n",
            "Epoch 31/200, Loss: 0.04744064807891846, Val Loss: 0.08664934803338754, Val MAE: 0.21882736680874096, Val RMSE: 0.29456912509964295\n",
            "Epoch 41/200, Loss: 0.012683162465691566, Val Loss: 0.07788128936609437, Val MAE: 0.20497616141906105, Val RMSE: 0.27921417836920265\n",
            "Epoch 51/200, Loss: 0.004023399669677019, Val Loss: 0.07193052341267897, Val MAE: 0.19516253917496387, Val RMSE: 0.2682905494939422\n",
            "Epoch 61/200, Loss: 0.0007780454470776021, Val Loss: 0.06810431091943918, Val MAE: 0.18891922547664314, Val RMSE: 0.26103817366361026\n",
            "Epoch 71/200, Loss: 0.0006814705557189882, Val Loss: 0.06584537363035141, Val MAE: 0.1850731497047805, Val RMSE: 0.25666655365993574\n",
            "Epoch 81/200, Loss: 0.000748270598705858, Val Loss: 0.0637701806219327, Val MAE: 0.18175821070184583, Val RMSE: 0.25258811015060595\n",
            "Epoch 91/200, Loss: 0.0007701756549067795, Val Loss: 0.06210891998270688, Val MAE: 0.1789973244850546, Val RMSE: 0.24928576597132873\n",
            "Epoch 101/200, Loss: 0.0007222964195534587, Val Loss: 0.061181813424636175, Val MAE: 0.1771071239790845, Val RMSE: 0.2474324765636501\n",
            "Epoch 111/200, Loss: 0.0012014639796689153, Val Loss: 0.059972975855273315, Val MAE: 0.17490695530252973, Val RMSE: 0.24498311601051062\n",
            "Epoch 121/200, Loss: 0.0015354218194261193, Val Loss: 0.05935426831589958, Val MAE: 0.1737229715306947, Val RMSE: 0.24372833346676057\n",
            "Epoch 131/200, Loss: 0.0016875627916306257, Val Loss: 0.05881262808564426, Val MAE: 0.17298077738877354, Val RMSE: 0.24262681665944666\n",
            "Epoch 141/200, Loss: 0.0014267924707382917, Val Loss: 0.058302327506028845, Val MAE: 0.17223169556182363, Val RMSE: 0.24158020738548805\n",
            "Epoch 151/200, Loss: 0.0018165141809731722, Val Loss: 0.05776898211912613, Val MAE: 0.17118943630620673, Val RMSE: 0.2404754908594317\n",
            "Epoch 161/200, Loss: 0.0022172576282173395, Val Loss: 0.05717631346529963, Val MAE: 0.17007349614580852, Val RMSE: 0.23924593963760238\n",
            "Epoch 171/200, Loss: 0.0021502578165382147, Val Loss: 0.056898087510742205, Val MAE: 0.16939432405514665, Val RMSE: 0.2386672385484071\n",
            "Epoch 181/200, Loss: 0.0020396907348185778, Val Loss: 0.05644794526601458, Val MAE: 0.16839592060810601, Val RMSE: 0.2377208911978322\n",
            "Epoch 191/200, Loss: 0.001828257110901177, Val Loss: 0.0561033742491565, Val MAE: 0.16752452577107466, Val RMSE: 0.23699204234548982\n",
            "Epoch 1/200, Loss: 0.6307146549224854, Val Loss: 0.3979430778350444, Val MAE: 0.5241151268364549, Val RMSE: 0.631014713299998\n",
            "Epoch 11/200, Loss: 0.17866098880767822, Val Loss: 0.149614170322873, Val MAE: 0.29781848719913845, Val RMSE: 0.3865392382950956\n",
            "Epoch 21/200, Loss: 0.10707207769155502, Val Loss: 0.11566755063616471, Val MAE: 0.2541228091529273, Val RMSE: 0.33967983737891105\n",
            "Epoch 31/200, Loss: 0.05240759998559952, Val Loss: 0.10050437681247733, Val MAE: 0.23584365442338806, Val RMSE: 0.3166821824292897\n",
            "Epoch 41/200, Loss: 0.019493473693728447, Val Loss: 0.08967974657095926, Val MAE: 0.2214525782391077, Val RMSE: 0.2992269006453473\n",
            "Epoch 51/200, Loss: 0.004563956055790186, Val Loss: 0.08249433885592257, Val MAE: 0.21105867849027207, Val RMSE: 0.28701593219859983\n",
            "Epoch 61/200, Loss: 0.0019207628211006522, Val Loss: 0.07528975478917188, Val MAE: 0.20005490445228655, Val RMSE: 0.2741689728520848\n",
            "Epoch 71/200, Loss: 0.002088664099574089, Val Loss: 0.07050347286195768, Val MAE: 0.1921714335118837, Val RMSE: 0.26526932833060163\n",
            "Epoch 81/200, Loss: 0.002811863087117672, Val Loss: 0.06609774818051757, Val MAE: 0.18511235873720705, Val RMSE: 0.25678468813690064\n",
            "Epoch 91/200, Loss: 0.003911294043064117, Val Loss: 0.06279160227691162, Val MAE: 0.18034204491403513, Val RMSE: 0.25017719823295925\n",
            "Epoch 101/200, Loss: 0.003628959646448493, Val Loss: 0.0611542033063883, Val MAE: 0.1780010855621911, Val RMSE: 0.24679575633756887\n",
            "Epoch 111/200, Loss: 0.0032005729153752327, Val Loss: 0.05982792666020421, Val MAE: 0.17590803891133036, Val RMSE: 0.24400328285738798\n",
            "Epoch 121/200, Loss: 0.002656960394233465, Val Loss: 0.05887832523813482, Val MAE: 0.17440067838186002, Val RMSE: 0.24199525865836805\n",
            "Epoch 131/200, Loss: 0.0023610282223671675, Val Loss: 0.058110043137318136, Val MAE: 0.17332554925360258, Val RMSE: 0.24041759107420818\n",
            "Epoch 141/200, Loss: 0.0021241321228444576, Val Loss: 0.05763545031578555, Val MAE: 0.1728654652856717, Val RMSE: 0.2394758991273833\n",
            "Epoch 151/200, Loss: 0.001931475242599845, Val Loss: 0.057090063573997145, Val MAE: 0.17210854957139018, Val RMSE: 0.23841669212989314\n",
            "Epoch 161/200, Loss: 0.002005171962082386, Val Loss: 0.0568276994856279, Val MAE: 0.17159743525313673, Val RMSE: 0.237935244168936\n",
            "Epoch 171/200, Loss: 0.002185436664149165, Val Loss: 0.05650068928266881, Val MAE: 0.17112987889181652, Val RMSE: 0.23732716681779392\n",
            "Epoch 181/200, Loss: 0.0023840568028390408, Val Loss: 0.05626826130413596, Val MAE: 0.1709029063161909, Val RMSE: 0.23694433000745477\n",
            "Epoch 191/200, Loss: 0.0022314353846013546, Val Loss: 0.05642157217024723, Val MAE: 0.17133618624283273, Val RMSE: 0.2373454364899588\n",
            "Epoch 1/200, Loss: 0.6454600095748901, Val Loss: 0.41173426643272354, Val MAE: 0.5285429809990589, Val RMSE: 0.6416671919258823\n",
            "Epoch 11/200, Loss: 0.13427650928497314, Val Loss: 0.13904966041445732, Val MAE: 0.287299199877989, Val RMSE: 0.3729435544370198\n",
            "Epoch 21/200, Loss: 0.06793881952762604, Val Loss: 0.10834793132767512, Val MAE: 0.24840036488304224, Val RMSE: 0.3291581927571532\n",
            "Epoch 31/200, Loss: 0.029256533831357956, Val Loss: 0.09610564001119895, Val MAE: 0.23098570389480208, Val RMSE: 0.30994764254352825\n",
            "Epoch 41/200, Loss: 0.010525692254304886, Val Loss: 0.08768579184018463, Val MAE: 0.21740751670125077, Val RMSE: 0.2960741176965909\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Evaluation on Test Set\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size)\n",
        "\n",
        "test_loss = 0\n",
        "test_predictions = []\n",
        "test_targets = []\n",
        "\n",
        "for X_batch, y_batch in test_dataset:\n",
        "    adj_matrix_test = tf.eye(X_batch.shape[1])  # Update adjacency matrix shape to match sequence length\n",
        "    test_outputs = model(X_batch, adj_matrix_test)\n",
        "    test_loss += loss_fn(y_batch, test_outputs).numpy()\n",
        "    test_predictions.append(test_outputs.numpy())\n",
        "    test_targets.append(y_batch.numpy())\n",
        "\n",
        "test_loss /= len(test_dataset)\n",
        "test_predictions = np.concatenate(test_predictions, axis=0)\n",
        "test_targets = np.concatenate(test_targets, axis=0)\n",
        "\n",
        "mae = mean_absolute_error(test_targets, test_predictions)\n",
        "rmse = np.sqrt(mean_squared_error(test_targets, test_predictions))\n",
        "r2 = r2_score(test_targets, test_predictions)"
      ],
      "metadata": {
        "id": "4EpXg7am6JWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Test MAE:\", mae)\n",
        "print(\"Test RMSE:\", rmse)\n",
        "print(\"Test R^2:\", r2)"
      ],
      "metadata": {
        "id": "UK7Us6n16crQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate NMAE and NRMSE\n",
        "mean_target = np.mean(test_targets)\n",
        "nmae = mae / mean_target\n",
        "nrmse = rmse / mean_target"
      ],
      "metadata": {
        "id": "CukHF1ME6hPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Test NMAE:\", nmae)\n",
        "print(\"Test NRMSE:\", nrmse)"
      ],
      "metadata": {
        "id": "qcbjtafG6lRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Actual vs Predicted values\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(test_targets, label='Actual')\n",
        "plt.plot(test_predictions, label='Predicted')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('ALLSKY_SFC_SW_DWN')\n",
        "plt.title('Actual vs forecasted Values')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kLCZfxaE6oNe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}